{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Langchain's Custom Chat Model Class Inheritance and method overriding for Minimax"
      ],
      "metadata": {
        "id": "D68rL1iWrFXo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![link text](https://avatars.githubusercontent.com/u/194880281?s=280&v=4)"
      ],
      "metadata": {
        "id": "K-oA7sTNzmzg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8RaMcNNTq6Hw"
      },
      "outputs": [],
      "source": [
        "from typing import Any, Dict, Iterator, List, Optional\n",
        "from langchain_core.callbacks import CallbackManagerForLLMRun\n",
        "from langchain_core.language_models import BaseChatModel\n",
        "from langchain_core.messages import AIMessage, AIMessageChunk, BaseMessage\n",
        "from langchain_core.messages.ai import UsageMetadata\n",
        "from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult\n",
        "from pydantic import Field, BaseModel, SecretStr\n",
        "from langchain_core.utils.utils import secret_from_env\n",
        "from openai import OpenAI\n",
        "\n",
        "class ChatMinimax(BaseChatModel):\n",
        "    model_name: str = Field(default=\"MiniMax-Text-01\")\n",
        "    temperature: Optional[float] = 0.5\n",
        "    max_tokens: Optional[int] = 1024\n",
        "    timeout: Optional[int] = None\n",
        "    stop: Optional[List[str]] = None\n",
        "    max_retries: int = 2\n",
        "    minimax_api_key: Optional[SecretStr] = Field(\n",
        "        alias=\"api_key\", default_factory=secret_from_env(\"MINIMAX_API_KEY\", default=None)\n",
        "    )\n",
        "\n",
        "    def _generate(\n",
        "        self,\n",
        "        messages: List[BaseMessage],\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> ChatResult:\n",
        "        client = OpenAI(api_key=self.minimax_api_key.get_secret_value(), base_url=\"https://api.minimaxi.chat/v1\")\n",
        "        response = client.chat.completions.create(\n",
        "            model=self.model_name,\n",
        "            messages=[{\"role\": \"user\", \"content\": message.content} for message in messages],\n",
        "            temperature=self.temperature,\n",
        "            max_tokens=self.max_tokens,\n",
        "        )\n",
        "\n",
        "        aimessage = AIMessage(\n",
        "            content=response.choices[0].message.content,\n",
        "            usage_metadata={\n",
        "                \"input_tokens\": response.usage.prompt_tokens,\n",
        "                \"output_tokens\": response.usage.completion_tokens,\n",
        "                \"total_tokens\": response.usage.total_tokens,\n",
        "            },\n",
        "        )\n",
        "\n",
        "        return ChatResult(generations=[ChatGeneration(message=aimessage)])\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"ChatMinimax(model_name='{self.model_name}', temperature={self.temperature})\"\n",
        "\n",
        "    def _stream(\n",
        "        self,\n",
        "        messages: List[BaseMessage],\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> Iterator[ChatGenerationChunk]:\n",
        "        client = OpenAI(api_key=self.minimax_api_key.get_secret_value(), base_url=\"https://api.minimaxi.chat/v1\")\n",
        "\n",
        "        stream = client.chat.completions.create(\n",
        "            model=self.model_name,\n",
        "            messages=[{\"role\": \"user\", \"content\": message.content} for message in messages],\n",
        "            temperature=self.temperature,\n",
        "            max_tokens=self.max_tokens,\n",
        "            stream=True,\n",
        "        )\n",
        "\n",
        "        for chunk in stream:\n",
        "            if chunk.choices[0].delta.content:\n",
        "                yield ChatGenerationChunk(\n",
        "                    message=AIMessageChunk(\n",
        "                        content=chunk.choices[0].delta.content\n",
        "                    )\n",
        "                )\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"Minimax-Text-01\"\n",
        "\n",
        "    @property\n",
        "    def _identifying_params(self) -> Dict[str, Any]:\n",
        "        return {\n",
        "            \"model_name\": self.model_name,\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Invoking calls (Asynchronous & Asynchronous)"
      ],
      "metadata": {
        "id": "xl_VYQuQrdtj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "model = ChatMinimax(model_name=\"MiniMax-Text-01\", temperature=0.7, max_output_tokens=2048,api_key=userdata.get(\"MINIMAX_API_KEY\"))\n",
        "model\n",
        "\n",
        "response = model.invoke(\"Hi how can I help you\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bimbeMcOrZ-P",
        "outputId": "5a712e1d-c6d9-489a-ced7-b2db97f353fa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Hi there! It’s great to hear from you. I’m here to help answer any questions you might have or engage in a conversation on any topic you’re interested in. Whether it’s about technology, hobbies, current events, or just a friendly chat, I’m all ears. How can I assist you today?' additional_kwargs={} response_metadata={} id='run-76b08514-8b35-424c-aba8-eda88197f85d-0' usage_metadata={'input_tokens': 749, 'output_tokens': 64, 'total_tokens': 813}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Streaming"
      ],
      "metadata": {
        "id": "9YECzNUUrqlQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in model.stream(\"Hi who are you\"):\n",
        "  print(chunk.content, end=\"\",flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jS3ZKahRrsBy",
        "outputId": "897e2f00-3310-400b-84e7-b1701c1eb0d0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! I am 海螺 AI, an AI assistant developed to help answer questions, provide information, and engage in discussions on a wide range of topics. I'm here to assist you with whatever you need, whether it's learning something new, solving a problem, or just having a friendly chat. How can I assist you today?"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unit Tests\n"
      ],
      "metadata": {
        "id": "Rk_PBYZRsX4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unittest\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "class TestChatMinimax(unittest.TestCase):\n",
        "    def test_chat_response(self):\n",
        "      model = ChatMinimax(model_name=\"MiniMax-Text-01\", temperature=0.7, max_output_tokens=2048,api_key=userdata.get(\"MINIMAX_API_KEY\"))\n",
        "      response = model.invoke(\"Hello how can I help you\")\n",
        "\n",
        "# Run tests manually\n",
        "suite = unittest.TestLoader().loadTestsFromTestCase(TestChatMinimax)\n",
        "unittest.TextTestRunner(verbosity=2).run(suite)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZY91OXhpsa6L",
        "outputId": "9f282eaf-ea53-4288-ae13-0ed81eafddf9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_chat_response (__main__.TestChatMinimax.test_chat_response) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 5.144s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import httpx\n",
        "from typing import Any, Dict, Iterator, List, Optional, Union\n",
        "from langchain_core.callbacks import CallbackManagerForLLMRun\n",
        "from langchain_core.language_models import BaseChatModel\n",
        "from langchain_core.messages import AIMessage, AIMessageChunk, BaseMessage\n",
        "from langchain_core.messages.ai import UsageMetadata\n",
        "from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult\n",
        "from pydantic import Field, SecretStr\n",
        "from langchain_core.utils.utils import secret_from_env\n",
        "from openai import OpenAI\n",
        "\n",
        "class ChatMinimax(BaseChatModel):\n",
        "    model_name: str = Field(default=\"MiniMax-Text-01\")\n",
        "    temperature: Optional[float] = 0.5\n",
        "    max_tokens: Optional[int] = 1024\n",
        "    timeout: Optional[int] = None\n",
        "    stop: Optional[List[str]] = None\n",
        "    max_retries: int = 2\n",
        "    minimax_api_key: Optional[SecretStr] = Field(\n",
        "        alias=\"api_key\", default_factory=secret_from_env(\"MINIMAX_API_KEY\", default=None)\n",
        "    )\n",
        "\n",
        "    @staticmethod\n",
        "    def encode_image(image_path: str) -> str:\n",
        "      if not image_path.startswith(\"http\"):\n",
        "        with open(image_path, \"rb\") as image_file:\n",
        "          return base64.b64encode(image_file.read()).decode('utf-8')\n",
        "      else:\n",
        "        response = httpx.get(image_path)\n",
        "        response.raise_for_status()\n",
        "        return base64.b64encode(response.content).decode('utf-8')\n",
        "\n",
        "    def _generate(\n",
        "        self,\n",
        "        messages: List[BaseMessage],\n",
        "        image: Optional[Union[str, bytes]] = None,\n",
        "        image_is_base64: bool = False,\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> ChatResult:\n",
        "        \"\"\"Generate a response from MiniMax's API, with optional image input.\"\"\"\n",
        "\n",
        "        client = OpenAI(api_key=self.minimax_api_key.get_secret_value(), base_url=\"https://api.minimaxi.chat/v1\")\n",
        "\n",
        "        # Format messages\n",
        "        formatted_messages = [\n",
        "            {\"role\": \"system\", \"content\": \"MM Intelligent Assistant is a self-developed MiniMax model.\"},\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"name\": \"user\",\n",
        "                \"content\": [{\"type\": \"text\", \"text\": msg.content} for msg in messages],\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        # Add image input if provided\n",
        "        if image:\n",
        "            image_payload = {\n",
        "                \"type\": \"image_url\",\n",
        "                \"image_url\": {\n",
        "                    \"url\": f\"data:image/jpeg;base64,{image}\" if image_is_base64 else image\n",
        "                }\n",
        "            }\n",
        "            formatted_messages[1][\"content\"].append(image_payload)\n",
        "\n",
        "        # API call\n",
        "        response = client.chat.completions.create(\n",
        "            model=self.model_name,\n",
        "            messages=formatted_messages,\n",
        "            temperature=self.temperature,\n",
        "            max_tokens=self.max_tokens,\n",
        "        )\n",
        "\n",
        "        aimessage = AIMessage(\n",
        "            content=response.choices[0].message.content,\n",
        "            usage_metadata=UsageMetadata(\n",
        "                input_tokens=response.usage.prompt_tokens,\n",
        "                output_tokens=response.usage.completion_tokens,\n",
        "                total_tokens=response.usage.total_tokens,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        return ChatResult(generations=[ChatGeneration(message=aimessage)])\n",
        "\n",
        "    def _stream(\n",
        "        self,\n",
        "        messages: List[BaseMessage],\n",
        "        image: Optional[Union[str, bytes]] = None,\n",
        "        image_is_base64: bool = False,\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> Iterator[ChatGenerationChunk]:\n",
        "        \"\"\"Stream responses from MiniMax API with optional image input.\"\"\"\n",
        "\n",
        "        client = OpenAI(api_key=self.minimax_api_key.get_secret_value(), base_url=\"https://api.minimaxi.chat/v1\")\n",
        "\n",
        "        formatted_messages = [\n",
        "            {\"role\": \"system\", \"content\": \"MM Intelligent Assistant is a self-developed MiniMax model.\"},\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"name\": \"user\",\n",
        "                \"content\": [{\"type\": \"text\", \"text\": msg.content} for msg in messages],\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        if image:\n",
        "            image_payload = {\n",
        "                \"type\": \"image_url\",\n",
        "                \"image_url\": {\n",
        "                    \"url\": f\"data:image/jpeg;base64,{image}\" if image_is_base64 else image\n",
        "                }\n",
        "            }\n",
        "            formatted_messages[1][\"content\"].append(image_payload)\n",
        "\n",
        "        stream = client.chat.completions.create(\n",
        "            model=self.model_name,\n",
        "            messages=formatted_messages,\n",
        "            temperature=self.temperature,\n",
        "            max_tokens=self.max_tokens,\n",
        "            stream=True,\n",
        "        )\n",
        "\n",
        "        for chunk in stream:\n",
        "            if chunk.choices[0].delta.content:\n",
        "                yield ChatGenerationChunk(\n",
        "                    message=AIMessageChunk(content=chunk.choices[0].delta.content)\n",
        "                )\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"Minimax-Text-01\"\n",
        "\n",
        "    @property\n",
        "    def _identifying_params(self) -> Dict[str, Any]:\n",
        "        return {\"model_name\": self.model_name}\n"
      ],
      "metadata": {
        "id": "XSsR8ragu6II"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Minimax's Multimodal Understanding on different types of images"
      ],
      "metadata": {
        "id": "SzInf4ZSypMh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "p2dtz7bTypJA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Google's Titan Architecture aiming to enhance cognitive memory mimicking that of human's"
      ],
      "metadata": {
        "id": "7zoXwWL8xUvP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![link text](https://miro.medium.com/v2/resize:fit:1400/1*_YLjLN1GDHtAFWhr3rW5Pw.png)"
      ],
      "metadata": {
        "id": "zqs0cErLxP7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_base64 = ChatMinimax.encode_image(\"https://miro.medium.com/v2/resize:fit:1400/1*_YLjLN1GDHtAFWhr3rW5Pw.png\")\n",
        "response = model.invoke(\"What is in this image?\", image=image_base64, image_is_base64=True)\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y497o7YPyAl1",
        "outputId": "d6852c99-0669-434b-e7c1-56c4664d2951"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='This image illustrates a neural memory architecture used in machine learning models, particularly those involving attention mechanisms and memory retrieval processes. It shows the flow of information between different components such as neural memory, core sequence, and persistent memory, and how they interact during learning and testing phases. The diagram highlights the retrieval and updating of neural memory, the sequence processing in the core, and the use of learnable data-independent weights in persistent memory.', additional_kwargs={}, response_metadata={}, id='run-791b1a2a-9400-4d2a-82a4-1cdc953f1040-0', usage_metadata={'input_tokens': 3723, 'output_tokens': 85, 'total_tokens': 3808})"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ghost of Tsushima Scenery\n",
        "\n",
        "![link text](https://i.ytimg.com/vi/U54zml7zwng/maxresdefault.jpg)"
      ],
      "metadata": {
        "id": "oCgC38SIzGD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_base64 = ChatMinimax.encode_image(\"https://i.ytimg.com/vi/U54zml7zwng/maxresdefault.jpg\")\n",
        "response = model.invoke(\"What is in this image?\", image=image_base64, image_is_base64=True)\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n90Zl1ezy7Jl",
        "outputId": "a13fa60a-84a3-44ce-f1b7-6fae0902b89c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"The image depicts a serene and picturesque scene of a large tree covered in white blossoms, situated on a small piece of land that juts out into a calm body of water. The tree is the central focus of the image, with its branches spreading wide and covered in delicate white flowers, suggesting that it might be a cherry blossom tree in full bloom. The tree trunk is dark and sturdy, contrasting with the lightness of the blossoms.\\n\\nAround the tree, there are a few scattered leaves in various colors, including orange and red, which could indicate the transition from spring to autumn or simply the natural shedding of leaves. The ground around the tree base appears slightly damp or muddy, suggesting recent rain or the proximity to the water.\\n\\nThe body of water surrounding the tree is calm and reflective, with a misty or foggy atmosphere that adds a dreamy and tranquil quality to the scene. This mist partially obscures the background, creating a sense of depth and mystery. The water's surface is relatively still, with only slight ripples disturbing its mirror-like quality.\\n\\nIn the background, there are several small wooden structures, possibly cabins or cottages, nestled among more trees that also appear to be in bloom. These structures are simple and rustic, blending harmoniously with the natural surroundings. The background trees are also covered in white blossoms, similar to the central tree, suggesting a landscape dominated by cherry blossoms or a similar flowering species.\\n\\nThe overall color palette of the image is soft and muted, with the white blossoms, misty atmosphere, and calm water creating a peaceful and almost ethereal ambiance. The scattered autumn leaves add a touch of warmth and contrast to the predominantly cool tones of the scene.\\n\\nThe image evokes a sense of tranquility and natural beauty, with the blooming tree as the centerpiece of a serene and idyllic landscape. The misty atmosphere and calm water enhance the feeling of peace and quiet, making it a visually soothing and aesthetically pleasing scene.\", additional_kwargs={}, response_metadata={}, id='run-26a203a7-2f70-44c3-bfdc-0a56fb1d4201-0', usage_metadata={'input_tokens': 5857, 'output_tokens': 383, 'total_tokens': 6240})"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    }
  ]
}